{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10188116,"sourceType":"datasetVersion","datasetId":6291207}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers PyPDF2 python-docx\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U sentence-transformers\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nimport PyPDF2\nimport textwrap\nimport docx\nfrom difflib import get_close_matches\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport re\n\n# Load advanced QA and summarization pipelines\nqa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\nsummarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # For semantic search\n\n# Cleaning function to preprocess text\ndef clean_text(text):\n    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = re.sub(r'Page \\d+ of \\d+|\\bheader text\\b|\\bfooter text\\b', '', text, flags=re.IGNORECASE)\n    text = re.sub(r'(\\b\\w+\\b)(?:\\s+\\1\\b)+', r'\\1', text, flags=re.IGNORECASE)\n    return text\n\n# Extract text from TXT\ndef extract_text_txt(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return file.read()\n\n# Extract text from PDF\ndef extract_text_pdf(file_path):\n    text = \"\"\n    with open(file_path, 'rb') as file:\n        reader = PyPDF2.PdfReader(file)\n        for page in reader.pages:\n            text += page.extract_text()\n    return text\n\n# Extract text from DOCX\ndef extract_text_docx(file_path):\n    doc = docx.Document(file_path)\n    text = \" \".join([paragraph.text for paragraph in doc.paragraphs])\n    return text\n\n# Unified function for processing\ndef process_document(file_path, file_type):\n    if file_type == \"txt\":\n        raw_text = extract_text_txt(file_path)\n    elif file_type == \"pdf\":\n        raw_text = extract_text_pdf(file_path)\n    elif file_type == \"docx\":\n        raw_text = extract_text_docx(file_path)\n    else:\n        raise ValueError(\"Unsupported file type\")\n    return clean_text(raw_text)\n\n# Answer a question\ndef answer_question(question, context):\n    result = qa_pipeline(question=question, context=context)\n    return result['answer'], result['score']\n\n# Semantic Search for Question Answering across multiple documents\ndef semantic_search(query, documents):\n    query_embedding = embedding_model.encode([query])\n    document_embeddings = [embedding_model.encode([doc]) for doc in documents.values()]\n    similarities = [np.dot(query_embedding, doc_emb.T) for doc_emb in document_embeddings]\n    most_similar_idx = np.argmax(similarities)\n    return list(documents.values())[most_similar_idx]\n\n# Handle typos and unclear inputs\ndef handle_typos(user_input, valid_commands):\n    close_matches = get_close_matches(user_input, valid_commands, n=1, cutoff=0.8)\n    return close_matches[0] if close_matches else None\n\n# Improved summarize function (handles long documents in chunks)\ndef summarize_document(text, max_length=150, min_length=50, summary_type=\"abstract\"):\n    # Handle chunking of large documents (for abstractive summarization)\n    def chunk_text(text, max_chunk_size=1024):\n        return textwrap.wrap(text, max_chunk_size, break_long_words=False)\n\n    if summary_type == \"abstract\":\n        chunks = chunk_text(text)\n        summaries = []\n        for chunk in chunks:\n            summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)\n            summaries.append(summary[0]['summary_text'])\n        return \" \".join(summaries)  \n    elif summary_type == \"extractive\":\n        sentences = text.split('.')\n        num_sentences = 5 \n        return '. '.join(sentences[:num_sentences]) + '.'\n    else:\n        raise ValueError(\"Unsupported summary type\")\n\n# Interactive chatbot with multiple document support\ndocuments = {}\nprint(\"Upload and load your documents first.\")\nwhile True:\n    command = input(\"Command (upload/view/switch/start/exit): \").lower()\n    if command == \"exit\":\n        print(\"Goodbye!\")\n        break\n    elif command == \"upload\":\n        file_path = input(\"Enter the file path: \")\n        file_type = input(\"Enter the file type (txt/pdf/docx): \").lower()\n        try:\n            doc_name = input(\"Enter a name for this document: \")\n            documents[doc_name] = process_document(file_path, file_type)\n            print(f\"Document '{doc_name}' uploaded and cleaned successfully.\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n    elif command == \"view\":\n        print(\"Available documents:\")\n        for name in documents.keys():\n            print(f\"- {name}\")\n    elif command == \"switch\":\n        doc_name = input(\"Enter the document name to switch to: \")\n        if doc_name in documents:\n            context = documents[doc_name]\n            print(f\"Switched to document '{doc_name}'.\")\n        else:\n            print(\"Document not found.\")\n    elif command == \"start\":\n        if not documents:\n            print(\"No documents uploaded. Please upload a document first.\")\n            continue\n        context = list(documents.values())[0]  # Default to the first document\n        print(\"Chatbot is ready! You can ask questions or request summaries.\")\n        while True:\n            user_input = input(\"Enter your question or command (type 'exit' to go back): \")\n            if user_input.lower() == \"exit\":\n                print(\"Exiting chatbot interaction.\")\n                break\n            elif \"summarize\" in user_input.lower():\n                summary_type = \"abstract\" if \"abstract\" in user_input.lower() else \"extractive\"\n                section = input(\"Enter section to summarize or leave blank for the whole document: \").lower()\n                if section:\n                    if section in context.lower():\n                        section_text = context[context.lower().index(section):]\n                        summary = summarize_document(section_text, summary_type=summary_type)\n                        print(f\"Summary of the section '{section}':\")\n                        print(summary)\n                    else:\n                        print(f\"Section '{section}' not found in the document.\")\n                else:\n                    summary = summarize_document(context, summary_type=summary_type)\n                    print(\"Summary of the document:\")\n                    print(summary)\n            else:\n                relevant_document = semantic_search(user_input, documents)\n                answer, confidence = answer_question(user_input, relevant_document)\n                if confidence < 0.0:\n                    print(\"I'm not confident about the answer. Can you clarify your question?\")\n                else:\n                    print(f\"Answer: {answer}\\nConfidence: {confidence:.2f}\\n\")\n    else:\n        suggested_command = handle_typos(command, [\"upload\", \"view\", \"switch\", \"start\", \"exit\"])\n        if suggested_command:\n            print(f\"Did you mean '{suggested_command}'?\")\n        else:\n            print(\"Invalid command. Please try again.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}